{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Jl3uq0gg-E"
      },
      "source": [
        "# **[인공지능] 과제4 Cliff Walking 예제 구현**\n",
        "*   **QLearning Class 및 Sarsa Class를 완성하여 결과를 살펴보는 것이 목표**입니다.\n",
        "*   기본적인 코드는 아래 노트에 모두 작성되어 있습니다. 비어있는 함수 부분을 완성하면 됩니다.\n",
        "*   **과제 수행 시 주의사항: 외부 라이브러리로 Q-learning 및 Sarsa 적용하지 말 것, 수업 때 배운 내용대로 Q-learning과 Sarsa를 주어진 함수에 구현할 것.** 웹 상에 있는 다양한 Q-learning 및 Sarsa 코드를 참고하는 것은 괜찮습니다.\n",
        "*   **보고서 작성 내용**: 여러분이 완성한 Q-learning 및 sarsa 알고리즘의 내용과 결과의 의미를 분석하는 내용을 작성하면 됩니다.\n",
        "작성한 코드와 실행 결과를 첨부하길 바라며, 코드에는 자세한 주석을 필수적으로 포함하기 바랍니다. 보고서는 PDF로 제출바랍니다.\n",
        "*   보고서는 12월 13일 오후 11시 59분까지 블랙보드에 보고서 형태로 제출하면 됩니다. 지각은 0점입니다.\n",
        "\n",
        "# **본 노트를 본인의 drive로 복사하여 활용하기 바랍니다.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame\n"
      ],
      "metadata": {
        "id": "6PiDYTreOYJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc4ce447-b0d9-4a40-abf4-db52691f0c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlwSKTZhgDTM"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, namedtuple, deque\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv # Cliff Walking 환경\n",
        "\n",
        "#추가한것\n",
        "import pandas as pd\n",
        "import gym\n",
        "\n",
        "#####\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "본 과제는 OpenAI Gym 환경에 기반하여 작성되었습니다. Gym 라이브러리는 학습을 적용할 수 있는 다양한 환경을 제공합니다. 여기서는 수업에서 다뤘던 Cliff Walking 환경을 활용합니다.\n",
        "\n",
        "Cliff Walking 예제의 state 개수는 48개, action 개수는 4개입니다."
      ],
      "metadata": {
        "id": "hvkZ159wRbAy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orqk6rcLg-uZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636b49ab-ad1c-4c8e-e52c-e09ad71c63ec"
      },
      "source": [
        "env = CliffWalkingEnv()\n",
        "print ('Number of states: ', env.nS)\n",
        "print ('Number of actions :', env.nA)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of states:  48\n",
            "Number of actions : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "printPolicy 함수는 학습된 Q-value를 입력하면 해당하는 Q-value의 greedy 정책이 출력되도록 합니다. Q-learning 및 Sarsa를 이용하여 학습된 정책을 출력하기 위해 필요합니다.\n",
        "\n",
        "* 함수의 입력값은 'Q[state][action]'의 형태로 정리된 행동가치 함수값 (48,4) 행렬이고, 입력에 따른 greedy 정책을 출력합니다.\n",
        "\n",
        "**구현되어있는 함수를 그대로 활용하면 됩니다.**"
      ],
      "metadata": {
        "id": "d4sO3ulcQkwi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLfl84A7nfo7"
      },
      "source": [
        "def printPolicy(self, Q):\n",
        "    action = ['↑', '→', '↓', '←', '×']\n",
        "    policy = np.array([np.argmax(Q[key]) \n",
        "                      if key in Q else -1 \n",
        "                      for key in np.arange(48)])\n",
        "    v = ([np.max(Q[key]) if key in Q else 0 for key in np.arange(48)])\n",
        "    actions = np.stack([action for _ in range(len(policy))], axis=0)\n",
        "\n",
        "    print(np.take(actions, np.reshape(policy, (4, 12))))\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "argmax는 최댓값을 반환한다. \n",
        "np.arange(시작점(생략 시 0), 끝점(미포함), step size(생략 시 1)) 인 array 생성 즉 0부터 47까지인 배열 생성하고 key 를 안에 넣어서 찾는다.\n",
        "\n",
        "\n",
        "즉 0부터 "
      ],
      "metadata": {
        "id": "zuwcF_hN-yPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning과 Sarsa 모두 behavior policy로 epsilon-greedy를 쓴다고 가정합니다."
      ],
      "metadata": {
        "id": "RFFwcX4gRTzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 Q-learning 알고리즘을 수행하는 Class의 정의입니다.\n",
        "*   `update()` 메쏘드의 경우 state, action, reward, next_state, next_action이 주어졌을 때 Q-value를 업데이트하는 함수입니다.\n",
        "*   `act()` 메쏘드의 경우 $\\epsilon$-greedy 정책에 따라 action을 선택하는 함수입니다."
      ],
      "metadata": {
        "id": "vZqrmNaKRY9A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aod3D2Lnkw7_"
      },
      "source": [
        "class QLearning:\n",
        "    def __init__(self):\n",
        "        self.action_no = 4\n",
        "        self.alpha = 0.01\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.5\n",
        "        self.q_values = defaultdict(lambda: [0.0] * self.action_no)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, next_action):\n",
        "        max_act = np.argmax(self.q_values[next_state])\n",
        "        # argmax는 최댓값을 반환한다. \n",
        "        old = self.q_values[state][action] # 현재의 상태는 old로 저장한다.\n",
        "        new = self.q_values[next_state][max_act] # \n",
        "        self.q_values[state][action] = (1-self.alpha) * old+ self.alpha * ( self.gamma * new+ reward)\n",
        "        #현재상태 (S, state), 현재행동 (A, action), 보상 (R, reward),\n",
        "        #  다음상태 (S', next_state), 다음행동 (A', next_action)\n",
        "        # 위 다섯가지 원소를 가지고 시간차(TD)를 학습합니다.\n",
        "        # SARSA와 다른점이 있다면 SARSA가 정책 반복의 샘플 기반 구현법이라면,\n",
        "        #Q러닝은 가치 반복의 샘플 기반 구현 기법이다.\n",
        "    def act(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_no)\n",
        "        else:\n",
        "            return np.argmax(self.q_values[state])\n",
        "      #상태정보를 넣으면 행동을 반환한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 Sarsa 알고리즘을 수행하는 Class의 정의입니다.\n",
        "*   `update()` 메쏘드의 경우 state, action, reward, next_state, next_action이 주어졌을 때 Q-value를 업데이트하는 함수입니다.\n",
        "*   `act()` 메쏘드의 경우 $\\epsilon$-greedy 정책에 따라 action을 선택하는 함수입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "cSd9yuf-ReVo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moiYtfA0pYiY"
      },
      "source": [
        "class Sarsa:\n",
        "    def __init__(self):\n",
        "        self.action_no = 4\n",
        "        self.alpha = 0.01\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.5\n",
        "        self.q_values = defaultdict(lambda: [0.0] * self.action_no)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, next_action):\n",
        "        new = self.q_values[next_state][next_action]\n",
        "        now = self.q_values[state][action]\n",
        "        self.q_values[state][action] =  self.alpha * (reward + self.gamma * new)+ (1 - self.alpha) * now \n",
        "        #업데이트 공식에 맞추어서 현재의 행동 큐함수와\n",
        "        #다음 행동 큐함수를 구하고 이를 바탕으로 현재의 행동 큐함수의 업데이트를 진행한한다.\n",
        "    def act(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_no)\n",
        "            #randrange()는 랜덤으로 무작위하게 숫자를 반환하는 함수이다. \n",
        "            #randrange(self.action_no)는 0 <= x < self.action_no 의 범위 내에서의 랜덤한 정수(int)를 반환합니다. \n",
        "        else:\n",
        "            return np.argmax(self.q_values[state])\n",
        "            #ϵ - 탐욕 정책에 따라 0에서 1사이의 실수 난수를 뽑아서 \n",
        "            #그 값이 epsilon 보다 작으면 행동중에서 무작위로 하나를\n",
        "            #뽑아 행동하고 아닐 경우 \n",
        "            #큐함수 테이블에서 해당 상태의 큐함수 리스트 중에서 가장 큰 값에 따라 행동한한다\n",
        "    # 상태정보를 넣으면 행동을 반환한다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning 및 Sarsa Class를 정의하고 5000 episode 동안 학습을 수행합니다.\n",
        "\n",
        "Gym 라이브러리의 환경에서는 `step(action)` 메쏘드를 통해 해당하는 time-step에서 action을 수행한 효과를 얻을 수 있습니다. 해당 메쏘드에서는 action을 수행하여 얻어지는 보상 (reward), 다음 상태 (next_state), done (episode 종료여부) 등이 출력으로 주어집니다."
      ],
      "metadata": {
        "id": "0uIbXLBVR_x_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "patW0RK5olea"
      },
      "source": [
        "agent_QL = QLearning()\n",
        "for ep in range(5000): # 5000번의 에피소드를 통해 학습합니다.\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    action = agent_QL.act(state)\n",
        "    \n",
        "    ep_rewards = 0\n",
        "    while not done:\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        # _은 값은 반환하지만 쓰지는 않겠다는 의미이다. \n",
        "        next_act = agent_QL.act(next_state)\n",
        "        # 다음 상태가 들어간 액션을 next action에 대입한다.\n",
        "        agent_QL.update(state, action, reward, next_state, next_act)\n",
        "        #업데이트를 해준다. \n",
        "        state = next_state\n",
        "        #다음 상태를 대입한다.\n",
        "        action = next_act\n",
        "        # 액션도 다음 액션을 넣어줌으로써 다음 행동을 준비한다.\n",
        "        ep_rewards += reward # 보상로 새로 update한다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv2LB-xEqYL4"
      },
      "source": [
        "agent_Sa = Sarsa()\n",
        "for ep in range(5000):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    action = agent_Sa.act(state)\n",
        "    \n",
        "    ep_rewards = 0\n",
        "    while not done:\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        # _은 값은 반환하지만 쓰지는 않겠다는 의미이다. \n",
        "        next_action = agent_Sa.act(next_state)\n",
        "        # 다음 상태가 들어간 액션을 next action에 대입한다\n",
        "        agent_Sa.update(state, action, reward, next_state, next_action)\n",
        "         #업데이트를 해준다. \n",
        "        state = next_state\n",
        "         #다음 상태를 대입한다.\n",
        "        action = next_action\n",
        "         # 액션도 다음 액션을 넣어줌으로써 다음 행동을 준비한다.\n",
        "        ep_rewards += reward\n",
        "        # reward를 보상해준다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습된 Q-value를 이용하여 학습된 정책을 출력합니다."
      ],
      "metadata": {
        "id": "b5yRRGCaSRHi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqHh3TRcs3dh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecefbf4c-958c-407b-e46b-1a9f080203fc"
      },
      "source": [
        "print('Learned policy by Q-learning')\n",
        "printPolicy(agent_QL ,agent_QL.q_values)\n",
        "print('Learned policy by Sarsa')\n",
        "printPolicy(agent_Sa,agent_Sa.q_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned policy by Q-learning\n",
            "[['→' '→' '→' '→' '↓' '→' '→' '→' '→' '→' '↓' '↓']\n",
            " ['→' '→' '→' '→' '↓' '↓' '↓' '→' '↓' '↓' '↓' '↓']\n",
            " ['→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '×' '×' '×' '×' '×' '×' '×' '×' '×' '×' '↑']]\n",
            "\n",
            "Learned policy by Sarsa\n",
            "[['→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '→' '↑' '↑' '↑' '↑' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '↑' '↑' '↑' '↑' '↑' '↑' '→' '→' '↑' '→' '↓']\n",
            " ['↑' '×' '×' '×' '×' '×' '×' '×' '×' '×' '×' '↑']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NFlcKhvoq4u"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}